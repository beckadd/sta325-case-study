---
title: "Case Study"
author: "Beck Addison and Marian Simo"
date: "10/12/2021"
output: pdf_document
---

```{r import-packages, message=FALSE}
library(tidyverse)
library(MASS)
```

```{r data-import, message=FALSE}
train <- read_csv('data/data-train.csv')
test <- read_csv('data/data-test.csv')
```

```{r data-manipulation}
train_aug <- train %>%
  mutate(
    mean = R_moment_1,
    variance = R_moment_2 - R_moment_1^2,
    skewness = R_moment_3^2/R_moment_2^2,
    kurtosis = R_moment_4/R_moment_2^2
  ) 

train_long <- train_aug %>%
  pivot_longer(cols = c(mean, variance, skewness, kurtosis), 
               names_to = 'statistic', values_to = 'statistic_value') %>%
  pivot_longer(cols = c(St, Re, Fr), 
               names_to = 'predictor', values_to = 'predictor_value') %>%
  mutate(
    statistic = fct_relevel(statistic, c('mean', 'variance', 'skewness', 'kurtosis')),
    predictor = fct_relevel(predictor, c('Re', 'St', 'Fr'))
  )
```

```{r EDA-chart-1, fig.width = 16}
train_long %>%
  ggplot(aes(
    x = as.factor(predictor_value),
    y = statistic_value,
    color = statistic
  )) +
  geom_boxplot() +
  facet_wrap(vars(predictor, statistic), scales = "free") +
  labs(
    title = "Distribution Statistics across values of Fr, St, and Re",
    x = "Fr",
    y = "Statistic Value"
  )
```

This table of graphs suggests the following key notes:

1. The mean, variance, and skewness of the distribution of cluster volumes drops significantly as `Re` increases, while kurtosis appears to increase (polynomially?).

2. There isn't a strong trend for `St` except to note that, as it increases, the maximum value of the mean, variance, and skewness increases fairly dramatically, but the kurtosis falls steadily. 

3. The mean of the particle cluster volume remains mostly consistent for values of `Fr`, but the variance and skewness drop quickly. The kurtosis with respect to `Fr` doesn't seem to have a clear trend.

Other things to note include:

1. The variance and skewness of the distribution doesn't seem to be dependent on `St`. The mean and particularly the kurtosis of the distribution does appear to have some relationship with `St`, however, though kurtosis seems more closely related than the variance in what appears to be a roughly linear decline.

2. `Re` evokes a strong change in all of the distribution statistics as it increases, sharply reducing the variance in their distributions. I'd hypothesize that `Re` has a quadratic or otherwise polynomial effect on the distribution of the particle size, since it appears to change the variance and skewness of the set so sharply. 

3. `Fr` evokes similar changes in the variance and skewness of the distribution as `Re`, but not in the kurtosis and mean. This could mean there is some interaction effect in the second moment of the data between `Re` and `St`.


```{r data-transformation, echo=FALSE}
# account for the inf in Fr to be able to model
train$Fr[train$Fr==Inf] <- 1
```

```{r linear-regression-R1, echo=FALSE, results=FALSE}
r1.lm.fit <- lm(R_moment_1 ~ St + Re + Fr, data = train)
summary(r1.lm.fit)
par(mfrow=c(2,2))
plot(r1.lm.fit, which=c(2,1))
```
```{r linear-regression-R2, echo=FALSE, results=FALSE}
r2.lm.fit <- lm(R_moment_2 ~ St + Re + Fr, data = train)
summary(r2.lm.fit)
par(mfrow=c(2,2))
plot(r2.lm.fit, which=c(2,1))
```


```{r linear-regression-R3, echo=FALSE, results=FALSE}
r3.lm.fit <- lm(R_moment_3 ~ St + Re + Fr, data = train)
summary(r3.lm.fit)
par(mfrow=c(2,2))
plot(r3.lm.fit, which=c(2,1))
```


```{r linear-regression-R4, echo=FALSE, results=FALSE}
r4.lm.fit <- lm(R_moment_4 ~ St + Re + Fr, data = train)
summary(r4.lm.fit)
par(mfrow=c(2,2))
plot(r4.lm.fit, which=c(2,1))
```

These attempts of linear regression suggests the following key notes:

1. Using the Residual Standard Error and Adjusted $R^2$, they suggest a simple linear regression model does not fit the data well. Raw moments 2, 3, and 4 suffer from a large RSE and very low $R^2$ values. Raw moment 1 has a higher $R^2$ value of 0.6271 and a much lower RSE value of 0.0341; however, those values are indicative of weak linearity.

2. Looking at the residual plots, for all 4 raw moments it appears a linear model does fit well up until larger values of the response variable. This could be indicative of a needed transformation for the response values.

3. The QQ plots for raw moments 2, 3, and 4 also suggest linearity up until the larger values of the response variables. The QQ plot of raw moment 1 follows a "S" pattern that could suggest under-dispersed data.

```{r non-linear-regression-attempt-R1, echo=FALSE, results=FALSE}
r1.nl.fit <- lm(R_moment_1 ~ polym(St, Re, Fr, degree=2), data = train)
summary(r1.nl.fit)
par(mfrow=c(2,2))
plot(r1.nl.fit, which=c(2,1))
```


```{r non-linear-regression-attempt-R2-R4, echo=FALSE, results=FALSE, fig.show='hide'}
r2.nl.fit <- lm(R_moment_2 ~ polym(St, Re, Fr, degree=2), data = train)
summary(r2.nl.fit)
r3.nl.fit <- lm(R_moment_3 ~ polym(St, Re, Fr, degree=2), data = train)
summary(r3.nl.fit)
r4.nl.fit <- lm(R_moment_4 ~ polym(St, Re, Fr, degree=2), data = train)
summary(r4.nl.fit)
par(mfrow=c(2,2))
plot(r3.nl.fit, which=c(2,1))
plot(r2.nl.fit, which=c(2,1))
plot(r4.nl.fit, which=c(2,1))
```

Attempting polynomial regression suggests the following key notes: 

1. Polynomial regression only benefited raw moment 1, increasing its adjusted $R^2$ to 0.9646 and reducing its RSE to 0.01051. The QQ plot now follows a more linear pattern and the residual vs fitted plot also better follows the line of best fit. Thus, this suggests a polynomial model is better suited for raw moment 1.

2. Raw moments 2, 3, and 4 did not benefit from a polynomial model, each still suffered from a low $R^2$ and very high RSE.

```{r R2 interaction terms, echo=FALSE, results=FALSE}
r2.lm.interact <- lm(R_moment_2 ~ .^2 , data = subset(train, select = -c(R_moment_1,R_moment_3, R_moment_4)))
stepAIC(r2.lm.interact,direction="backward")
```


```{r R3 interaction terms, echo=FALSE, results=FALSE}
r3.lm.interact <- lm(R_moment_3 ~ .^2 , data = subset(train, select = -c(R_moment_1,R_moment_2, R_moment_4)))
stepAIC(r3.lm.interact,direction="backward")
```


```{r R4 interaction terms, echo=FALSE, results=FALSE}
r4.lm.interact <- lm(R_moment_4 ~ .^2 , data = subset(train, select = -c(R_moment_1,R_moment_2, R_moment_3)))
stepAIC(r4.lm.interact,direction="backward")
```


Important takeaways from backward selection:

1. In all three raw moments for which backward selection was done, St and Fr resulted in an interaction term as well as Re and Fr.  

2. Both raw moment 3 and 4 contain all 3 interaction terms whereas raw moment 2 leaves out the interaction between St and Re.

