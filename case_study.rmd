---
title: "Case Study"
author: "Beck Addison and Marian Simo"
date: "10/12/2021"
output:
  pdf_document: default
bibliography: case_study_refs.bib
---

```{r global-options, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r import-packages, message=FALSE}
library(tidyverse)
library(broom)
library(MASS)
```

```{r data-import, message=FALSE}
train <- read_csv('data/data-train.csv')
test <- read_csv('data/data-test.csv')
```

```{r data-manipulation, message=FALSE, warning=FALSE}
train_aug <- train %>%
  mutate(
    mean = R_moment_1,
    variance = R_moment_2 - R_moment_1^2,
    skewness = (R_moment_3/R_moment_2)^2,
    kurtosis = R_moment_4/R_moment_2^2
  ) 

train_long <- train_aug %>%
  pivot_longer(cols = c(mean, variance, skewness, kurtosis), 
               names_to = 'statistic', values_to = 'statistic_value') %>%
  pivot_longer(cols = c(St, Re, Fr), 
               names_to = 'predictor', values_to = 'predictor_value') %>%
  mutate(
    statistic = fct_relevel(statistic, c('mean', 'variance', 'skewness', 'kurtosis')),
    predictor = fct_relevel(predictor, c('Re', 'St', 'Fr'))
  )
```

```{r data-transformation}
# account for the inf in Fr to be able to model
train_aug <- train_aug %>%
  mutate(
    Fr = case_when(
    Fr == Inf ~ 1,
    TRUE ~ Fr)
  )
```

```{r regressions, message=FALSE, include=FALSE}
moment_regression_fits <- train_aug %>%
  dplyr::select(!c(mean, variance, skewness, kurtosis)) %>%
  pivot_longer(cols = starts_with('R_moment'), names_to = 'moment', values_to = 'moment_value') %>%
  group_by(moment) %>%
  nest() %>%
  mutate( # generate linear models
    lm.fit.no_interactions = map(data, ~ lm(moment_value ~ St + Re + Fr, .)),
    lm.fit.two_way = map(data, ~ stepAIC(lm(moment_value ~ .^2, .), direction = 'backward')),
    nl.fit.two_degree = map(data, ~ lm(moment_value ~ polym(St, Re, Fr, degree=2), .))
  ) %>%
  mutate( # generate summaries from the linear models
    across(starts_with('lm') | starts_with('nl'), ~map(.x, ~summary(.x)), .names = '{.col}.summary')
  )

statistic_regression_fits <- train_aug %>%
  dplyr::select(!starts_with('R_moment')) %>%
  pivot_longer(cols = !c(Re, St, Fr), names_to = 'statistic', values_to = 'statistic_value') %>%
  group_by(statistic) %>%
  nest() %>%
  mutate( # generate linear models
    lm.fit.no_interactions = map(data, ~ lm(statistic_value ~ St + Re + Fr, .)),
    lm.fit.two_way = map(data, ~ stepAIC(lm(statistic_value ~ .^2, .), direction = 'backward')),
    nl.fit.two_degree = map(data, ~ lm(statistic_value ~ polym(St, Re, Fr, degree=2), .))
  ) %>%
  mutate( # generate summaries from the linear models
    across(starts_with('lm') | starts_with('nl'), ~map(.x, ~summary(.x)), .names = '{.col}.summary')
  )
```
# Introduction

The task of modeling turbulent fluid systems has consistently been one of the most difficult problems to solve in modern mathematics and physics; in fact, despite over two centuries of concerted research into the topic, no analytical method currently exists to accurately predict the evolution of turbulent systems over time.

Nevertheless, the successful simulation of turbulent systems remains uniquely essential to a diverse array of sciences and industries with common use cases including modeling airflow in aerodynamic design, analyzing blood flow in medicine [@blood-flow], and meteorological forecasting [@meteorological-forecasting]. To perform this simulation, high-resolution direct numerical simulation (DNS) is typically performed using the Navier-Stokes equation, but this is often very complex and computationally expensive to perform on any arbitrary set of inputs. In this case study, we circumvent these time-intensive simulation techniques by interpolating a set of results collected from Navier-Stokes DNS to produce a predictive model given a set of input parameters. Our objectives are therefore as follows:

- Given a set of training and test data describing the inputs of a DNS simulation, namely the Reynolds Number $Re$, the gravitational acceleration $Fr$, and the Stokes number $St$, generate a model or set of models that can accurately predict the particle cluster volume distribution in terms of four moments used as dependent variables. We will call this our *prediction* objective.

- Given the set of input and output parameters, determine how each parameter the distribution of cluster volumes. We will call this our *inference* objective.

# Methodology

To fulfill the objectives we pose in our introduction, we must collect more information about our modeling objectives. First, we have to define cluster volume in terms of the moments we are given. Since each simulation in our dataset represents a single row of three inputs $<Re, Fr, St>$ and four outputs $<E[X], E[X^2], E[X^3], E[X^4]>$ representing information about a *distribution* of several cluster volumes, we must consider that we are measuring the change in shape of a distribution of volumes rather than the volumes themselves. For this analysis, we defined key characteristics about these models in terms of these moments:

1. Mean: $E[X]$
2. Variance: $E[X^2] - E[X]^2$
3. Skewness: $(\frac{E[X^3]}{E[X^2]})^2$
4. Kurtosis: $\frac{E[X^4]}{E[X^2]^2}$

Our goal here is not only to make interpretations of our model predictions simpler but also to attempt to find some distribution that connects these outputs to our input parameters. To do this, we must first examine how these model characteristics are distributed in our dataset.

Nevertheless, we performed the same analysis on the raw moments in parallel to avoid oversimplifying the problem and so that we could provide correct predicted values (since we are predicting raw moments, *not* these statistics).

```{r EDA-chart-1, fig.height = 10, fig.width = 16}
train_long %>%
  ggplot(aes(
    x = as.factor(predictor_value),
    y = statistic_value,
    color = statistic
  )) +
  geom_boxplot() +
  facet_wrap(vars(predictor, statistic), scales = "free") +
  labs(
    title = "Figure 1. Distribution Statistics across values of Fr, St, and Re",
    x = "Fr",
    y = "Statistic Value"
  )
```
We noted from our EDA that the variance and skewness of the dataset appeared to drop dramatically with increasing values of $Fr$ and of $Re$, and that there seemed to be a possible interaction between 



# References
<div id = 'refs'></div>

# Appendix A: Notes on the EDA chart

This table of graphs suggests the following key notes:

1. The mean, variance, and skewness of the distribution of cluster volumes drops significantly as `Re` increases, while kurtosis appears to increase.

2. There isn't a strong trend for `St` except to note that, as it increases, the maximum value of the mean, variance, and skewness increases fairly dramatically, but the kurtosis falls steadily. 

3. The mean of the particle cluster volume remains mostly consistent for values of `Fr`, but the variance and skewness drop quickly. The kurtosis with respect to `Fr` doesn't seem to have a clear trend.

Other things to note include:

1. The variance and skewness of the distribution don't seem to be dependent on `St`. The mean and particularly the kurtosis of the distribution does appear to have some relationship with `St`, however, though kurtosis seems more closely related than the variance in what appears to be a roughly linear decline.

2. `Re` evokes a strong change in all of the distribution statistics as it increases, sharply reducing the variance in their distributions. I'd hypothesize that `Re` has a quadratic or otherwise polynomial effect on the distribution of the particle size, since it appears to change the variance and skewness of the set so sharply. 

3. `Fr` evokes similar changes in the variance and skewness of the distribution as `Re`, but not in the kurtosis and mean. This could mean there is some interaction effect in the second moment of the data between `Re` and `St`.

# Appendix B: Residual and Q-Q Plots 

## Raw Moments as Response Variables

```{r moment-data}
data = moment_regression_fits
names = data$moment
```

### Linear Models, No Interactions

```{r lm-plots-moments, fig.height= 10}
par(mfrow = c(4, 2))
pwalk(list(data$lm.fit.no_interactions, names), ~ plot(..1, which = c(2, 1)))

walk(data$lm.fit.no_interactions, ~tidy(summary(.x)))
```

In the graphs above, we've plotted the residuals and Normal Q-Q plots for all of our linear regressions where each of the raw moments represents a response variable to the three predictors $<Re, Fr, St>$. The first row represents information with the first raw moment as the response, the second row represents the second raw moment, and so on. For this first assessment, we have no interaction variables.

These attempts of linear regression suggests the following key notes:

1. Using the Residual Standard Error and Adjusted $R^2$, they suggest a simple linear regression model does not fit the data well. Raw moments 2, 3, and 4 suffer from a large RSE and very low $R^2$ values. Raw moment 1 has a higher $R^2$ value of 0.6271 and a much lower RSE value of 0.0341; however, those values are indicative of weak linearity.

2. Looking at the residual plots, for all 4 raw moments it appears a linear model does fit well up until larger values of the response variable. This could be indicative of a needed transformation for the response values.

3. The QQ plots for raw moments 2, 3, and 4 also suggest linearity up until the larger values of the response variables. The QQ plot of raw moment 1 follows a "S" pattern that could suggest under-dispersed data.

### Linear Models, 2-Way interactions with backward stepwise selection optimizing AIC

```{r two-way-plots-moments, fig.height= 10}
par(mfrow = c(4, 2))
pwalk(list(data$lm.fit.two_way, names), ~ plot(..1, which = c(2, 1)))
```

Important takeaways from backward selection:

1. In all three raw moments for which backward selection was done, St and Fr resulted in an interaction term as well as Re and Fr.  

2. Both raw moment 3 and 4 contain all 3 interaction terms whereas raw moment 2 leaves out the interaction between St and Re.


### Nonlinear Models, 2-degree polynomials

```{r nonlinear-plots-moments, fig.height= 10}
par(mfrow = c(4, 2))
pwalk(list(data$nl.fit.two_degree, names), ~ plot(..1, which = c(2, 1)))
```

Here again, we now compare a set of 2-degree nonlinear polynomial regression models, again with each of the raw moments as response variables to the predictor set from before.

Attempting polynomial regression suggests the following key notes: 

1. Polynomial regression only benefited raw moment 1, increasing its adjusted $R^2$ to 0.9646 and reducing its RSE to 0.01051. The QQ plot now follows a more linear pattern and the residual vs fitted plot also better follows the line of best fit. Thus, this suggests a polynomial model is better suited for raw moment 1.

2. Raw moments 2, 3, and 4 did not benefit from a polynomial model, each still suffered from a low $R^2$ and very high RSE.

## Mean/Variance/Skewness/Kurtosis as Response Variables

```{r stat-data}
data = statistic_regression_fits
names = data$statistic
```

### Linear Models, No Interactions

```{r lm-plots-stats, fig.height= 10}
par(mfrow = c(4, 2))
pwalk(list(data$lm.fit.no_interactions, names), ~ plot(..1, which = c(2, 1)))
```

### Linear Models, 2-Way interactions with backward stepwise selection optimizing AIC

```{r two-way-plots-stats, fig.height= 10}
par(mfrow = c(4, 2))
pwalk(list(data$lm.fit.two_way, names), ~ plot(..1, which = c(2, 1)))
```

### Nonlinear Models, 2-degree polynomials

```{r nonlinear-plots-stats, fig.height= 10}
par(mfrow = c(4, 2))
pwalk(list(data$nl.fit.two_degree, names), ~ plot(..1, which = c(2, 1)))
```
